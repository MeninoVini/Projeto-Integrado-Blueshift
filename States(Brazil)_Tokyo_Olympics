# Databricks notebook source
from pyspark.sql.types import *

# COMMAND ----------

list_uf = ['AC', 'AL', 'AP', 'AM', 'BA', 'CE', 'DF', 'ES', 'GO', 'MA', 'MT', 'MS', 'MG', 'PA', 'PB', 'PR', 'PE', 'PI', 'RJ', 'RN', 'RS', 'RO', 'RR', 'SC', 'SP', 'SE', 'TO']

# COMMAND ----------

list_estados = ['Acre', 'Alagoas', 'Amapa', 'Amazonas', 'Bahia', 'Ceara', 'Distrito Federal', 'Espirito Santo', 'Goias', 'Maranhao', 'Mato Grosso', 'Mato Grosso do Sul', 'Minas Gerais', 'Para', 'Paraiba', 'Parana', 'Pernambuco', 'Piaui', 'Rio de Janeiro', 'Rio Grande do Norte', 'Rio Grande do Sul', 'Rondonia', 'Roraima', 'Santa Catarina', 'Sao Paulo', 'Sergipe', 'Tocantins']

# COMMAND ----------

ziplist = zip(list_uf, list_estados)

schema = StructType([
  StructField('UF', StringType(), True),
  StructField('estados', StringType(), True)
])

df_final = (spark.createDataFrame(ziplist, schema=schema))

# COMMAND ----------

df_final.display()

# COMMAND ----------

df_final.repartition(1).write.option("header",True).mode("overwrite").option('sep',';').csv("/FileStore/producao/estados_sem_acento.csv")

# COMMAND ----------

storage_account_access_key = ""

spark.conf.set(
  "fs.azure.account.key.storagegrupoa.blob.core.windows.net",
  storage_account_access_key)

# COMMAND ----------

dbutils.fs.cp('/FileStore/producao/estados_sem_acento.csv/part-00000-tid-4011679188137767242-ffb0e43f-63b5-4e78-bc2a-eccad150766c-8-1-c000.csv','wasbs://conteinergrupoa@storagegrupoa.blob.core.windows.net//estados_sem_acento.csv', True)
